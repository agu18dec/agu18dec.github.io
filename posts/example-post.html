<!-- posts/example-post.html -->
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Building LLMs: From Theory to Practice | Agam Bhatia</title>
    <link rel="stylesheet" href="../style.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Playfair+Display:wght@400;500;600;700&family=Inter:wght@300;400;500&display=swap" rel="stylesheet">
    <!-- Add highlight.js for code syntax highlighting -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>
</head>
<body>
    <header>
        <div class="container">
            <a href="../index.html" class="logo">Agam Bhatia</a>
            <nav>
                <ul>
                    <li><a href="../index.html">Home</a></li>
                    <li><a href="../blog.html">Blog</a></li>
                </ul>
            </nav>
        </div>
    </header>

    <main>
        <article class="blog-post">
            <div class="container">
                <div class="blog-meta">
                    <span class="blog-date">March 27, 2025</span>
                    <span class="blog-category">AI</span>
                </div>
                <h1>Building LLMs: From Theory to Practice</h1>
                
                <div class="blog-content" id="markdown-content">
                    <!-- This div will be populated with the rendered Markdown -->
                </div>
            </div>
        </article>
    </main>

    <footer>
        <div class="container">
            <p>&copy; 2025 Agam Bhatia</p>
        </div>
    </footer>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/marked/4.3.0/marked.min.js"></script>
    <script>
        // The Markdown content is stored here
        const markdownContent = `
# Building LLMs: From Theory to Practice

Large Language Models (LLMs) have revolutionized natural language processing, enabling systems that can understand and generate human-like text at an unprecedented scale. In this post, I'll walk through the fundamentals of how these models work and provide a practical guide for implementing and fine-tuning them.

## The Architecture Behind LLMs

At their core, most modern LLMs are based on the Transformer architecture introduced in the landmark paper ["Attention Is All You Need"](https://arxiv.org/abs/1706.03762). This architecture relies on a mechanism called *self-attention* to process input sequences in parallel, rather than sequentially as in traditional RNNs.

The key components include:

- **Token Embeddings**: Converting words into vector representations
- **Positional Encodings**: Adding information about token positions
- **Multi-Head Attention**: Allowing the model to focus on different parts of the input
- **Feed-Forward Networks**: Processing the attention outputs
- **Layer Normalization**: Stabilizing the learning process

## Implementing a Simple Transformer

Here's a simplified implementation of a transformer block in PyTorch:

\`\`\`python
import torch
import torch.nn as nn

class SelfAttention(nn.Module):
    def __init__(self, embed_size, heads):
        super(SelfAttention, self).__init__()
        self.embed_size = embed_size
        self.heads = heads
        self.head_dim = embed_size // heads
        
        assert (self.head_dim * heads == embed_size), "Embed size needs to be divisible by heads"
        
        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)
        
    def forward(self, values, keys, query, mask):
        N = query.shape[0]
        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]
        
        # Split the embedding into self.heads pieces
        values = values.reshape(N, value_len, self.heads, self.head_dim)
        keys = keys.reshape(N, key_len, self.heads, self.head_dim)
        queries = query.reshape(N, query_len, self.heads, self.head_dim)
        
        values = self.values(values)
        keys = self.keys(keys)
        queries = self.queries(queries)
        
        # Scaled dot-product attention
        energy = torch.einsum("nqhd,nkhd->nqkh", [queries, keys])
        
        if mask is not None:
            energy = energy.masked_fill(mask == 0, float("-1e20"))
            
        attention = torch.softmax(energy / (self.embed_size ** (1/2)), dim=2)
        
        out = torch.einsum("nqkh,nvhd->nqvd", [attention, values]).reshape(
            N, query_len, self.heads * self.head_dim
        )
        
        out = self.fc_out(out)
        return out
\`\`\`

## Fine-Tuning Strategies

Fine-tuning an LLM for specific domains involves several key considerations:

1. **Dataset Preparation**: Curating high-quality, domain-specific data
2. **Parameter-Efficient Fine-Tuning**: Using techniques like LoRA (Low-Rank Adaptation) to update only a small subset of parameters
3. **Evaluation Metrics**: Defining appropriate metrics for your specific task

### Example of LoRA Implementation

\`\`\`python
class LoRALayer(nn.Module):
    def __init__(self, in_dim, out_dim, rank=8, alpha=16):
        super().__init__()
        self.A = nn.Parameter(torch.randn(in_dim, rank) * 0.01)
        self.B = nn.Parameter(torch.zeros(rank, out_dim))
        self.alpha = alpha
        self.rank = rank
        
    def forward(self, x):
        return (x @ self.A @ self.B) * (self.alpha / self.rank)
        
class LoRALinear(nn.Module):
    def __init__(self, linear_layer, rank=8, alpha=16):
        super().__init__()
        self.linear = linear_layer
        self.lora = LoRALayer(
            linear_layer.in_features, 
            linear_layer.out_features,
            rank=rank,
            alpha=alpha
        )
        
    def forward(self, x):
        return self.linear(x) + self.lora(x)
\`\`\`

## Lessons from Practical Applications

In my experience working with LLMs at Stanford NLP, I've found that:

- The quality of training data is often more important than model size
- Domain-specific fine-tuning can yield dramatic improvements over general-purpose models
- Careful instruction tuning is critical for getting consistent results
- Evaluation should include both automated metrics and human feedback

## Conclusion

Building and fine-tuning LLMs is both an art and a science. While the theoretical foundations are well-established, successful implementation requires careful attention to data quality, training procedures, and evaluation methodologies.

In future posts, I'll dive deeper into specific techniques for optimizing LLM performance for specialized domains and tasks.
        `;

        // Render the Markdown content
        document.addEventListener('DOMContentLoaded', function() {
            const markdownDiv = document.getElementById('markdown-content');
            marked.setOptions({
                highlight: function(code, lang) {
                    if (lang && hljs.getLanguage(lang)) {
                        return hljs.highlight(code, { language: lang }).value;
                    }
                    return hljs.highlightAuto(code).value;
                },
                breaks: true,
                gfm: true
            });
            
            // Convert Markdown to HTML and insert into the page
            markdownDiv.innerHTML = marked.parse(markdownContent);
            
            // Initialize highlight.js for syntax highlighting
            document.querySelectorAll('pre code').forEach((block) => {
                hljs.highlightBlock(block);
            });
        });
    </script>
</body>
</html>
